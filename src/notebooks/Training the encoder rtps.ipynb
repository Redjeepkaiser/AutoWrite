{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e702dcc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11983, 1713, 4) (11983, 64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "padded_features = np.load(\"../../data/processed_data/rtp_features_padded.npy\")\n",
    "padded_target = np.load(\"../../data/processed_data/target_padded.npy\")\n",
    "\n",
    "with open(\"../../data/processed_data/alphabet\", \"rb\") as f:\n",
    "    alphabet = pickle.load(f)\n",
    "\n",
    "print(padded_features.shape, padded_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12630381",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b619fe7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.9.0\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "print(tf.test.is_built_with_cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "523333b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(features, target, train_size=0.9):\n",
    "    size = len(features)\n",
    "    indices = np.arange(size)\n",
    "    np.random.shuffle(indices)\n",
    "    train_samples = int(size * train_size)\n",
    "    \n",
    "    x_train, y_train = features[indices[:train_samples]], target[indices[:train_samples]]\n",
    "    x_valid, y_valid = features[indices[train_samples:]], target[indices[train_samples:]]\n",
    "    \n",
    "    return (\n",
    "        x_train,\n",
    "        x_valid,\n",
    "        y_train,\n",
    "        y_valid,\n",
    "    )\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = split_data(test_features, padded_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc93ff2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10784 1713 4\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train), len(x_train[0]), len(x_train[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf601632",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_SIZE = len(alphabet) + 1\n",
    "\n",
    "N_BLSTM_LAYERS = 5\n",
    "N_CELLS = 64\n",
    "\n",
    "LEARNING_RATE = 10**-3\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "N_FEATURES = len(x_train[0][0])\n",
    "N_TIMESTEPS = 1713\n",
    "N_EPOCHS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82ebdeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Bidirectional, LSTM, Dense, Input, Masking, TimeDistributed\n",
    "\n",
    "def build_model(n_blstm_layers, n_cells, n_features, output_size):\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    model.add(Masking(input_shape=(None, N_FEATURES), mask_value=np.zeros((N_FEATURES))))\n",
    "    \n",
    "    for i in range(n_blstm_layers):\n",
    "        model.add(Bidirectional(LSTM(N_CELLS,\n",
    "                                     input_shape=(None, N_FEATURES),\n",
    "                                     return_sequences = True,\n",
    "                                     dropout = 0.5),\n",
    "                                merge_mode = 'sum'))\n",
    "\n",
    "    model.add(\n",
    "        TimeDistributed(\n",
    "            Dense(output_size, activation = 'softmax')\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d14c905",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_loss(y_true, y_pred):\n",
    "    batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n",
    "    input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n",
    "    label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n",
    "\n",
    "    input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "    label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "\n",
    "    loss = keras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ac82452",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(N_BLSTM_LAYERS, N_CELLS, N_FEATURES, OUTPUT_SIZE)\n",
    "\n",
    "model.compile(\n",
    "    loss=ctc_loss,\n",
    "    optimizer= tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE, clipnorm=9)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54760b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(train_data, valid_data, n_epochs, batch_size):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((train_data, valid_data))\n",
    "    dataset = dataset.shuffle(1000)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(1)\n",
    "    return dataset\n",
    "\n",
    "train_dataset = create_dataset(x_train, y_train, N_EPOCHS, BATCH_SIZE)\n",
    "validation_dataset = create_dataset(x_valid, y_valid, N_EPOCHS, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a55006cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "loss_history = []\n",
    "\n",
    "if Path(\"../../training/encoder/loss\").is_file():\n",
    "    with open(\"../../training/encoder/loss\", \"rb\") as f:\n",
    "        loss_history = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dde094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1348/1348 [==============================] - ETA: 0s - loss: 193.3737"
     ]
    }
   ],
   "source": [
    "# %%capture stored_output\n",
    "\n",
    "import keras\n",
    "\n",
    "checkpoint_path = \"../../training/encoder/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "if Path(checkpoint_path).is_file():\n",
    "    model.load_weights(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "\n",
    "logdir = '../../training/encoder/logs'\n",
    "tb_callback = tf.keras.callbacks.TensorBoard(logdir, update_freq=1)\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data = validation_dataset,\n",
    "    shuffle=True,\n",
    "    epochs = N_EPOCHS,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    callbacks = [cp_callback, tb_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13f277c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stored_output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe22bb7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss_history.append([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153f8f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../training/encoder/loss\", \"wb\") as f:\n",
    "    pickle.dump(loss_history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14b6510",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b863927",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "loss = []\n",
    "val_loss = []\n",
    "\n",
    "for training_session in total_history:\n",
    "    print(training_session.history)\n",
    "    loss.append(training_session.history['loss'])\n",
    "    val_loss.append(training_session.history['val_loss'])\n",
    "\n",
    "    \n",
    "def flatten_list(l):\n",
    "     return [point for elem in l for point in elem]\n",
    "\n",
    "plt.plot(flatten_list(loss), color=\"red\", label=\"train\")\n",
    "plt.plot(flatten_list(val_loss), color=\"black\", label=\"test\")\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78afdae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train:[100.6691, 95.1948]\n",
    "valid:[97.5810, 91.9827]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
