{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e702dcc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "padded_features = np.load(\"../../data/processed_data/bezier_features_padded_improved.npy\")\n",
    "padded_target = np.load(\"../../data/processed_data/target_padded.npy\")\n",
    "\n",
    "with open(\"../../data/processed_data/alphabet\", \"rb\") as f:\n",
    "    alphabet = pickle.load(f)\n",
    "\n",
    "print(padded_features.shape, padded_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12630381",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b619fe7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "print(tf.test.is_built_with_cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523333b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(features, target, train_size=0.9):\n",
    "    size = len(features)\n",
    "    indices = np.arange(size)\n",
    "    np.random.shuffle(indices)\n",
    "    train_samples = int(size * train_size)\n",
    "    \n",
    "    x_train, y_train = features[indices[:train_samples]], target[indices[:train_samples]]\n",
    "    x_valid, y_valid = features[indices[train_samples:]], target[indices[train_samples:]]\n",
    "    \n",
    "    return (\n",
    "        x_train,\n",
    "        x_valid,\n",
    "        y_train,\n",
    "        y_valid,\n",
    "    )\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = split_data(padded_features, padded_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc93ff2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(x_train), len(x_train[0]), len(x_train[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf601632",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_SIZE = len(alphabet) + 1\n",
    "\n",
    "N_BLSTM_LAYERS = 5\n",
    "N_CELLS = 64\n",
    "\n",
    "LEARNING_RATE = 10**-4\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "N_FEATURES = len(x_train[0][0])\n",
    "N_TIMESTEPS = len(x_train[0])\n",
    "N_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ebdeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Bidirectional, LSTM, Dense, Input, Masking, TimeDistributed\n",
    "\n",
    "def build_model(n_blstm_layers, n_cells, n_features, output_size):\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    model.add(Masking(input_shape=(None, N_FEATURES), mask_value=np.zeros((N_FEATURES))))\n",
    "    \n",
    "    for i in range(n_blstm_layers):\n",
    "        model.add(Bidirectional(LSTM(N_CELLS,\n",
    "                                     input_shape=(None, N_FEATURES),\n",
    "                                     return_sequences = True,\n",
    "                                     dropout = 0.5),\n",
    "                                merge_mode = 'sum'))\n",
    "\n",
    "    model.add(\n",
    "        TimeDistributed(\n",
    "            Dense(output_size, activation = 'softmax')\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d14c905",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_loss(y_true, y_pred):\n",
    "    batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n",
    "    input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n",
    "    label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n",
    "\n",
    "    input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "    label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "\n",
    "    loss = keras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac82452",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(N_BLSTM_LAYERS, N_CELLS, N_FEATURES, OUTPUT_SIZE)\n",
    "\n",
    "model.compile(\n",
    "    loss=ctc_loss,\n",
    "    optimizer= tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE, clipnorm=9)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54760b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(train_data, valid_data, n_epochs, batch_size):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((train_data, valid_data))\n",
    "    dataset = dataset.shuffle(1000)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(1)\n",
    "    return dataset\n",
    "\n",
    "train_dataset = create_dataset(x_train, y_train, N_EPOCHS, BATCH_SIZE)\n",
    "validation_dataset = create_dataset(x_valid, y_valid, N_EPOCHS, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55006cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "loss_history = [[], []]\n",
    "\n",
    "loss_path = Path(\"../../training/logs/encoder_bezier/loss\")\n",
    "if loss_path.is_file():\n",
    "    with open(loss_path, \"rb\") as f:\n",
    "        loss_history = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dde094",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "checkpoint_path = \"../../training/checkpoints/encoder_bezier/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "model.load_weights(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data = validation_dataset,\n",
    "    shuffle=True,\n",
    "    epochs = N_EPOCHS,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    callbacks = [cp_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe22bb7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "loss_history[0].extend(history.history['loss'])\n",
    "loss_history[1].extend(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153f8f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(loss_path, \"wb\") as f:\n",
    "    pickle.dump(loss_history, f)\n",
    "    \n",
    "print(len(loss_history[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b863927",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def flatten_list(l):\n",
    "     return [point for elem in l for point in elem]\n",
    "\n",
    "plt.plot(range(1000, 1000 + len(loss_history[0])), loss_history[0], color=\"red\", label=\"train\")\n",
    "plt.plot(range(1000, 1000 + len(loss_history[0])), loss_history[1], color=\"black\", label=\"test\")\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de90ae1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../autowrite\")\n",
    "from Visualizer import Visualizer\n",
    "\n",
    "visualizer = Visualizer()\n",
    "visualizer.plot_bezier_curves(padded_features[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fd565b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(loss_history[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e23ccec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "if Path(\"./../../data/processed_data/alphabet\").is_file():\n",
    "    with open(\"./../../data/processed_data/alphabet\", \"rb\") as f:\n",
    "        alphabet = pickle.load(f)\n",
    "\n",
    "print(alphabet, len(alphabet))\n",
    "\n",
    "def encode_textline(textline, alphabet):\n",
    "    return [alphabet.index(c) for c in textline]\n",
    "\n",
    "def decode_textline(encodedline, alphabet):\n",
    "    return [alphabet[int(v)] if int(v) < len(alphabet) else \"blank\"for v in encodedline]\n",
    "\n",
    "def remove_blanks(l):\n",
    "    return [elem for elem in l if elem != \"blank\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf91e9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Preprocessor import Preprocessor\n",
    "preprocessor = Preprocessor(\"../../data/processed_data/alphabet\")\n",
    "\n",
    "path = \"./input_example.npy\"\n",
    "raw_strokes = np.load(path, allow_pickle=True)\n",
    "p = preprocessor.strokes_to_bezier(raw_strokes, precision=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208c7d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = p\n",
    "output = model.call(tf.convert_to_tensor(np.expand_dims(sample, 0)), mask=None)\n",
    "print(output.shape)\n",
    "res = output.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659abff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "argmax = np.argmax(res, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d517c07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"\".join(remove_blanks(decode_textline(padded_target[i], alphabet))))\n",
    "\n",
    "print(\"\".join(remove_blanks(decode_textline(argmax, alphabet))))\n",
    "\n",
    "visualizer.plot_bezier_curves(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4a29c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped_output = tf.reshape(output, (output.shape[1], output.shape[0], output.shape[2]))\n",
    "(decoded, log_probabilities) = tf.nn.ctc_beam_search_decoder(reshaped_output, [reshaped_output.shape[0]], beam_width=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4c81cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decode_textline(decoded[0].values, alphabet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759fb1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kenlm\n",
    "from pyctcdecode import build_ctcdecoder\n",
    "\n",
    "# load unigram list\n",
    "with open(\"librispeech-vocab.txt\") as f:\n",
    "    unigram_list = [t for t in f.read().strip().split(\"\\n\")]\n",
    "\n",
    "decoder = build_ctcdecoder(\n",
    "    alphabet,\n",
    ")\n",
    "text = decoder.decode(res)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4ceeef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
