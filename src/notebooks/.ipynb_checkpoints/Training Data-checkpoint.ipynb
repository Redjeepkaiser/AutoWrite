{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f36f940",
   "metadata": {},
   "source": [
    "# Training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da62f3e",
   "metadata": {},
   "source": [
    "## Reading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1feb1c",
   "metadata": {},
   "source": [
    "In this notebook we will load the data that is used for training and convert it to the input representation that we need to train the network. The data is acquired here: \"https://fki.tic.heia-fr.ch/databases/iam-on-line-handwriting-database\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff2edbd",
   "metadata": {},
   "source": [
    "The dataset consists of two seperate directories. One directory contains $x$ and $y$ coordinates and corresponding timestamps in xml format for each seperate line of a text. The other directory contains the texts. These two directories are unpacked in the \"./data/raw_data\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b5fce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "path_strokefiles = Path(\"./../../data/raw_data/strokefiles\")\n",
    "path_textfiles = Path(\"./../../data/raw_data/textfiles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908b9ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "textfile_paths = [Path(dirpath + \"/\" + filenames[0])\n",
    "                  for (dirpath, dirnames, filenames) in os.walk(path_textfiles)\n",
    "                  if filenames != []] \n",
    "\n",
    "print(textfile_paths[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2284e3",
   "metadata": {},
   "source": [
    "First a function is defined to read the text lines from a text file, as these files need to be parsed and split up into lines, this is done with regex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88d278d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "example_textfile_path = textfile_paths[0]\n",
    "print(f'exmaple path = {example_textfile_path}\\n')\n",
    "with open(example_textfile_path) as f:\n",
    "    print(f.read())\n",
    "\n",
    "def get_file_lines(textfile_path):\n",
    "    f = open(textfile_path) \n",
    "    content = f.read()\n",
    "    lines = re.search(\"CSR:\\s*([^~]*)\", content).group(1).strip().split(\"\\n\")\n",
    "    return lines\n",
    "    \n",
    "example_textlines = get_file_lines(example_textfile_path)\n",
    "print(example_textlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e876ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke_paths = [(dirpath, filenames)\n",
    "                for (dirpath, dirnames, filenames) in os.walk(path_strokefiles)\n",
    "                if filenames != []]\n",
    "\n",
    "print(stroke_paths[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dc2468",
   "metadata": {},
   "source": [
    "Next, the paths of the xml files containing the $x$ and $y$ coordinates and timestamps for the lines in a certain text file are retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8174fcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stroke_paths(textfile_path):\n",
    "    strokefiles_root_folder = path_strokefiles / textfile_path.parts[-3] / textfile_path.parts[-2]\n",
    "    \n",
    "    if not strokefiles_root_folder.is_dir():\n",
    "        return None\n",
    "    \n",
    "    m = re.search(\"(.*?)-(.*)\", textfile_path.stem)\n",
    "        \n",
    "    res = [strokefiles_root_folder / filename for filename in sorted(os.listdir(strokefiles_root_folder))         \n",
    "           if re.search(\"(.*?)-(.*?)-.*\", filename).groups() == m.groups()]\n",
    "\n",
    "    if len(res) == 0:\n",
    "        return None\n",
    "    \n",
    "    return res\n",
    "\n",
    "example_textline_strokefile_paths = get_stroke_paths(example_textfile_path)\n",
    "print(example_textline_strokefile_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2888706",
   "metadata": {},
   "source": [
    "These files are then parse using the xml.etree.ElementTree module, this is a efficient python module that provides APIs to parse xml file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ff9b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "\n",
    "def read_file(filename):\n",
    "    root = ET.parse(filename).getroot()\n",
    "    \n",
    "    strokes = [[[point.attrib[\"x\"], point.attrib[\"y\"], point.attrib[\"time\"]]\n",
    "                 for point in stroke.findall(\"./Point\")]\n",
    "                for stroke in root.findall(\"./StrokeSet/Stroke\")]\n",
    "\n",
    "    max_stroke_len = max(len(r) for r in strokes)\n",
    "    \n",
    "    s = np.zeros((len(strokes), max_stroke_len, 3))\n",
    "    s[:, :, 2] -= 1\n",
    "\n",
    "    for i, row in enumerate(strokes):\n",
    "        s[i, :len(row)] = row\n",
    "\n",
    "    return s\n",
    "\n",
    "example_textline_strokes = [read_file(file) for file in example_textline_strokefile_paths]\n",
    "print(example_textline_strokes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3877097b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_strokes(strokes):\n",
    "    for stroke in strokes:\n",
    "        plt.plot(stroke[:, 0][stroke[:, 2] >= 0], stroke[:, 1][stroke[:, 2] >= 0])\n",
    "\n",
    "    plt.show()\n",
    "        \n",
    "for (textline, textline_strokes) in zip(example_textlines[:1], example_textline_strokes[:1]):\n",
    "    print(textline)\n",
    "    plot_strokes(textline_strokes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9bd861",
   "metadata": {},
   "source": [
    "## Encoding the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5950e1e",
   "metadata": {},
   "source": [
    "The encoding alphabet is made by taking looking at all unique characters in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ad22bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def get_alphabet(textfile_paths):\n",
    "    all_chars = set()\n",
    "    \n",
    "    for file in textfile_paths:\n",
    "        with open(file) as f:\n",
    "            content = f.read()\n",
    "            text_chars = set(re.search(\"CSR:\\s*([^~]*)\", content).group(1).strip())\n",
    "            \n",
    "            all_chars = all_chars.union(text_chars)\n",
    "        \n",
    "    return list(all_chars)\n",
    "\n",
    "alphabet = get_alphabet(textfile_paths)\n",
    "\n",
    "if Path(\"./../../data/processed_data/alphabet\").is_file():\n",
    "    with open(\"./../../data/processed_data/alphabet\", \"rb\") as f:\n",
    "        alphabet = pickle.load(f)\n",
    "\n",
    "print(alphabet, len(alphabet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb53e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_textline(textline, alphabet):\n",
    "    return [alphabet.index(c) for c in textline]\n",
    "\n",
    "def decode_textline(encodedline, alphabet):\n",
    "    return [alphabet[v] for v in encodedline]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad7905d",
   "metadata": {},
   "source": [
    "Now all the data is normalized and stored as explained in the notebook Input representation. These function are imported using import_ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f45d3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "from Input_Representation import (\n",
    "    normalize_strokes,\n",
    "    convert_to_rtps,\n",
    "    resample_strokes,\n",
    "    SSE,\n",
    "    makeSMatrix,\n",
    "    newton_step,\n",
    "    get_relative_distances,\n",
    "    fit_curve_newton_step,\n",
    "    fit_datapoints,\n",
    "    get_control_points,\n",
    "    parameterize_curve,\n",
    "    length_vecs,\n",
    "    dot_vecs,\n",
    "    calc_angles,\n",
    "    split_datapoints,\n",
    "    stitch_curves,\n",
    "    convert_stroke_to_bezier_curves,\n",
    "    plot_bezier_curves,\n",
    "    plot_rtps,\n",
    "    strokes_to_bezier,\n",
    "    scale_timestamps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a512db",
   "metadata": {},
   "outputs": [],
   "source": [
    "rtp_features = []\n",
    "bezier_features = []\n",
    "target = []\n",
    "\n",
    "for i, textfile_path in enumerate(textfile_paths):\n",
    "    if i % 100 == 0:\n",
    "        print(i/len(textfile_paths))\n",
    "\n",
    "    textline_strokefile_paths = get_stroke_paths(textfile_path)\n",
    "    \n",
    "    if not textline_strokefile_paths:\n",
    "        continue\n",
    "\n",
    "    lines = [encode_textline(line, alphabet) for line in get_file_lines(textfile_path)]\n",
    "    rtp_strokes = []\n",
    "    bezier_strokes = []\n",
    "    \n",
    "    for i, textfile_stroke_path in enumerate(textline_strokefile_paths):\n",
    "        rtp_strokes.append(convert_to_rtps(read_file(textfile_stroke_path)))\n",
    "        bezier_strokes.append(strokes_to_bezier(read_file(textfile_stroke_path), precision=0.01))\n",
    "\n",
    "    rtp_features.extend(rtp_strokes)\n",
    "    bezier_features.extend(bezier_strokes)\n",
    "    target.extend(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa86d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(rtp_features), len(target))\n",
    "\n",
    "s = 0\n",
    "\n",
    "for elem in bezier_features:\n",
    "    s+= len(elem)\n",
    "    \n",
    "print(s/len(rtp_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5539349",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = 1\n",
    "\n",
    "plot_rtps(rtp_features[sample])\n",
    "\n",
    "print(decode_textline(target[sample], alphabet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978678fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bezier_curves(bezier_features[sample][:30])\n",
    "\n",
    "print(decode_textline(target[sample], alphabet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e210734",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(rtp_features[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b90395",
   "metadata": {},
   "source": [
    "## Storing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776b424a",
   "metadata": {},
   "source": [
    "The data is then padded and stored as compressed numpy files using np.save(). The alphabet is also stored for later use when training the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d74bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_data(l, value=0, width=None):\n",
    "    max_len = max(len(item) for item in l)\n",
    "    \n",
    "    padded_numpy_array = None\n",
    "    if width:\n",
    "        padded_numpy_array = np.full((len(l), max_len, width), value, dtype=np.float32)\n",
    "    else:\n",
    "        padded_numpy_array = np.full((len(l), max_len), value, dtype=np.float32)\n",
    "    \n",
    "    for i, row in enumerate(l):\n",
    "        padded_numpy_array[i, :len(row)] = row\n",
    "    \n",
    "    return padded_numpy_array\n",
    "        \n",
    "padded_bezier_features = pad_data(bezier_features, width=11)\n",
    "print(padded_bezier_features[0])\n",
    "np.save(\"../../data/processed_data/bezier_features_padded_high_precision\", padded_bezier_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afeac04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"../../data/processed_data/alphabet\", \"wb\") as f:\n",
    "    pickle.dump(alphabet, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
