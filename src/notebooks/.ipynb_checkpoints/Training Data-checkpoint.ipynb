{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f36f940",
   "metadata": {},
   "source": [
    "# Training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da62f3e",
   "metadata": {},
   "source": [
    "## Reading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1feb1c",
   "metadata": {},
   "source": [
    "In this notebook we will load the data that is used for training and convert it to the input representation that we need to train the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff2edbd",
   "metadata": {},
   "source": [
    "The dataset consists of two seperate directories. One directory contains $x$ and $y$ coordinates and corresponding timestamps in xml format for each seperate line of a text. The other directory contains the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b5fce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "path_strokefiles = Path(\"./../../data/raw_data/strokefiles\")\n",
    "path_textfiles = Path(\"./../../data/raw_data/textfiles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908b9ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "textfile_paths = [Path(dirpath + \"/\" + filenames[0])\n",
    "                  for (dirpath, dirnames, filenames) in os.walk(path_textfiles)\n",
    "                  if filenames != []] \n",
    "\n",
    "print(textfile_paths[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2284e3",
   "metadata": {},
   "source": [
    "First a function is defined to read the text lines from a text file, as these files need to be parsed and split up into lines, this is done with regex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88d278d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "example_textfile_path = textfile_paths[0]\n",
    "print(f'exmaple path = {example_textfile_path}\\n')\n",
    "with open(example_textfile_path) as f:\n",
    "    print(f.read())\n",
    "\n",
    "def get_file_lines(textfile_path):\n",
    "    f = open(textfile_path) \n",
    "    content = f.read()\n",
    "    lines = re.search(\"CSR:\\s*([^~]*)\", content).group(1).strip().split(\"\\n\")\n",
    "    return lines\n",
    "    \n",
    "example_textlines = get_file_lines(example_textfile_path)\n",
    "print(example_textlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e876ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke_paths = [(dirpath, filenames)\n",
    "                for (dirpath, dirnames, filenames) in os.walk(path_strokefiles)\n",
    "                if filenames != []]\n",
    "\n",
    "print(stroke_paths[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dc2468",
   "metadata": {},
   "source": [
    "Next, the paths of the xml files containing the $x$ and $y$ coordinates and timestamps for the lines in a certain text file are retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8174fcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stroke_paths(textfile_path):\n",
    "    strokefiles_root_folder = path_strokefiles / textfile_path.parts[-3] / textfile_path.parts[-2]\n",
    "    \n",
    "    if not strokefiles_root_folder.is_dir():\n",
    "        return None\n",
    "    \n",
    "    m = re.search(\"(.*?)-(.*)\", textfile_path.stem)\n",
    "        \n",
    "    res = [strokefiles_root_folder / filename for filename in sorted(os.listdir(strokefiles_root_folder))         \n",
    "           if re.search(\"(.*?)-(.*?)-.*\", filename).groups() == m.groups()]\n",
    "\n",
    "    if len(res) == 0:\n",
    "        return None\n",
    "    \n",
    "    return res\n",
    "\n",
    "example_textline_strokefile_paths = get_stroke_paths(example_textfile_path)\n",
    "print(example_textline_strokefile_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2888706",
   "metadata": {},
   "source": [
    "These files are then parse using the xml.etree.ElementTree module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ff9b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "\n",
    "def read_file(filename):\n",
    "    root = ET.parse(filename).getroot()\n",
    "    \n",
    "    strokes = [[[point.attrib[\"x\"], point.attrib[\"y\"], point.attrib[\"time\"]]\n",
    "                 for point in stroke.findall(\"./Point\")]\n",
    "                for stroke in root.findall(\"./StrokeSet/Stroke\")]\n",
    "\n",
    "    max_stroke_len = max(len(r) for r in strokes)\n",
    "    \n",
    "    s = np.zeros((len(strokes), max_stroke_len, 3))\n",
    "    s[:, :, 2] -= 1\n",
    "\n",
    "    for i, row in enumerate(strokes):\n",
    "        s[i, :len(row)] = row\n",
    "\n",
    "    return s\n",
    "\n",
    "example_textline_strokes = [read_file(file) for file in example_textline_strokefile_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3877097b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_strokes(strokes):\n",
    "    for stroke in strokes:\n",
    "        plt.plot(stroke[:, 0][stroke[:, 2] >= 0], stroke[:, 1][stroke[:, 2] >= 0])\n",
    "\n",
    "    plt.show()\n",
    "        \n",
    "for (textline, textline_strokes) in zip(example_textlines, example_textline_strokes):\n",
    "    print(textline)\n",
    "    plot_strokes(textline_strokes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9bd861",
   "metadata": {},
   "source": [
    "## Encoding the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5950e1e",
   "metadata": {},
   "source": [
    "The encoding alphabet is made by taking looking at all unique characters in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ad22bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alphabet(textfile_paths):\n",
    "    all_chars = set()\n",
    "    \n",
    "    for file in textfile_paths:\n",
    "        with open(file) as f:\n",
    "            content = f.read()\n",
    "            text_chars = set(re.search(\"CSR:\\s*([^~]*)\", content).group(1).strip())\n",
    "            \n",
    "            all_chars = all_chars.union(text_chars)\n",
    "        \n",
    "    return list(all_chars)\n",
    "\n",
    "alphabet = get_alphabet(textfile_paths)\n",
    "print(alphabet, len(alphabet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb53e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_textline(textline, alphabet):\n",
    "    return [alphabet.index(c) for c in textline]\n",
    "\n",
    "def decode_textline(encodedline, alphabet):\n",
    "    return [alphabet[v] for v in encodedline]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad7905d",
   "metadata": {},
   "source": [
    "Now all the data is normalized and stored as explained in the notebook Input representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f495da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_strokes(strokes):\n",
    "    ustrokes = [np.unique(stroke[:, :2], return_index=True, axis=0)[1] for stroke in strokes]\n",
    "    max_stroke_len = max(len(r) for r in ustrokes)\n",
    "\n",
    "    normalized_strokes = np.zeros((len(ustrokes), max_stroke_len, 3))\n",
    "    normalized_strokes[:, :, 2] -= 1\n",
    "\n",
    "    for i, row in enumerate(ustrokes):\n",
    "        normalized_strokes[i, :len(row)] = strokes[i, np.sort(row)]\n",
    "\n",
    "    non_ragged = normalized_strokes[:, :, 2] >= 0\n",
    "    normalized_strokes[non_ragged] -= [normalized_strokes[0, 0, 0], np.amax(normalized_strokes[:, :, 1]), normalized_strokes[0, 0, 2]]\n",
    "    normalized_strokes[non_ragged] /= [-np.amin(normalized_strokes[:, :, 1]), np.amin(normalized_strokes[:, :, 1]), 1]\n",
    "\n",
    "    return normalized_strokes\n",
    "\n",
    "def sample_line(p0, p1, delta=0.05):\n",
    "    l = ((p1[0]-p0[0])**2 + (p1[1]-p0[1])**2)**0.5\n",
    "    num = int(l/delta)\n",
    "    \n",
    "    if num == 0:\n",
    "        return [p0]\n",
    "    \n",
    "    sampled_xs = np.linspace(p0[0], p1[0], num)\n",
    "    sampled_ys = np.linspace(p0[1], p1[1], num)\n",
    "    sampled_timestamps = np.linspace(p0[2], p1[2], num)\n",
    "\n",
    "    return np.stack((sampled_xs, sampled_ys, sampled_timestamps), axis=1).tolist()\n",
    "\n",
    "def resample_stroke(stroke):\n",
    "    resampled_stroke = []\n",
    "    \n",
    "    for i, _ in enumerate(stroke[stroke[:, 2] >= 0][:-1]):\n",
    "        resampled_stroke.extend(sample_line(stroke[i], stroke[i+1]))\n",
    "        \n",
    "    return resampled_stroke\n",
    "\n",
    "def resample_strokes(strokes):\n",
    "    rs = [resample_stroke(stroke) for stroke in strokes]\n",
    "    max_stroke_len = max(len(r) for r in rs)\n",
    "\n",
    "    resampled_strokes = np.zeros((len(rs), max_stroke_len, 3))\n",
    "    resampled_strokes[:, :, 2] -= 1\n",
    "\n",
    "    for i, row in enumerate(rs):\n",
    "        if row:\n",
    "            resampled_strokes[i, :len(row)] = row\n",
    "    \n",
    "    return resampled_strokes\n",
    "\n",
    "def add_extra_params(strokes):\n",
    "    directions = np.apply_along_axis(lambda x: int(x[x >= 0][0] < x[x >= 0][-1]), 1, strokes[:,:,1])\n",
    "    directions = np.tile(np.expand_dims(directions, axis=0).transpose(), (1, strokes.shape[1]))\n",
    "    rtps = np.append(strokes, np.expand_dims(directions, axis=2), axis=2)\n",
    "    rtps = np.append(rtps, np.zeros((strokes.shape[0], strokes.shape[1], 1)), axis=2)\n",
    "    rtps[:, 0, 4] = 1\n",
    "    touch_points = rtps[rtps[:, :, 2] >= 0].tolist()\n",
    "    return np.array(touch_points)\n",
    "\n",
    "def convert_stroke_to_bezier_curves(datapoints):\n",
    "    fitted_curves = fit_datapoints(datapoints)\n",
    "    stiched_curves = stitch_curves(fitted_curves)    \n",
    "    parameters = [parameterize_curve(PE, 1) for (PE, s, stdev, d) in stiched_curves]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38398b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeSMatrix(s, width): \n",
    "    return np.column_stack([s**p for p in range(width)])\n",
    "\n",
    "def SSE(data, P, s):\n",
    "    D = data[data[:, 2] >= 0]\n",
    "    S = makeSMatrix(s, 4)\n",
    "    return np.sum(np.sum((D - (S@P))**2, axis=1), axis=0)\n",
    "\n",
    "\n",
    "def newton_step(data, P, s):\n",
    "    D = data[data[:, 2] >= 0]\n",
    "    S = makeSMatrix(s, 4)\n",
    "    C = S@P\n",
    "\n",
    "    P1d = P[1:, :] * [[1], [2], [3]]\n",
    "    C1d = makeSMatrix(s, 3)@P1d # First derivates\n",
    "    \n",
    "    P2d = P1d[1:, :] * [[1], [2]]\n",
    "    C2d = makeSMatrix(s, 2)@P2d # Second derivates\n",
    "    \n",
    "    P3d = P2d[1:, :]\n",
    "    C3d = makeSMatrix(s, 1)@P3d # Third derivates\n",
    "    \n",
    "    N1 = (D[:, 0] - C[:, 0])*C2d[:, 0] + (D[:, 1] - C[:, 1])*C2d[:, 1]\\\n",
    "            - C2d[:, 0]**2 - C2d[:, 1]**2\n",
    "    \n",
    "    N2 = (D[:, 0] - C[:, 0])*C3d[:, 0] + (D[:, 1] - C[:, 1])*C3d[:, 1]\\\n",
    "            - 2*C3d[:, 0]*C2d[:, 0] - C1d[:, 0]*C2d[:, 0]\\\n",
    "            - 2*C3d[:, 1]*C2d[:, 1] - C1d[:, 1]*C2d[:, 1]\n",
    "    \n",
    "    s_new = np.copy(s)\n",
    "    s_new[1:-1] -= (N1/N2)[1:-1] # Keep s=0 and s=1 in place.\n",
    "    return s_new\n",
    "\n",
    "def get_relative_distances(data):\n",
    "    if len(data) < 2:\n",
    "        return np.array([[0.5]])\n",
    "\n",
    "    diffs = (data[1:, :] - data[:-1, :])\n",
    "    distances = np.insert((diffs[:, 0]**2 + diffs[:, 1]**2)**(1/2), 0, 0)\n",
    "    cummulative_distances = np.cumsum(distances)\n",
    "    return cummulative_distances/cummulative_distances[-1]\n",
    "\n",
    "def fit_curve_newton_step(data, delta=0.05, precision=0.05, maxiter=50):\n",
    "    D = data[data[:, 2] >= 0]\n",
    "    \n",
    "    if len(D) == 0:\n",
    "        return None\n",
    "    \n",
    "    s = get_relative_distances(D)\n",
    "    S = makeSMatrix(s, 4)\n",
    "    PE = np.linalg.lstsq(S, D, rcond=None)[0]\n",
    "    \n",
    "    prev_error = SSE(D, PE, s)\n",
    "    \n",
    "    if prev_error < precision:\n",
    "        return PE, s, prev_error\n",
    "    \n",
    "    for value in range(maxiter):\n",
    "        s = newton_step(D, PE, s)\n",
    "        S = makeSMatrix(s, 4)\n",
    "        PE = np.linalg.lstsq(S, D, rcond=None)[0]\n",
    "        \n",
    "        error = SSE(D, PE, s)\n",
    "\n",
    "        if abs(error - prev_error) < delta:\n",
    "            break\n",
    "\n",
    "        prev_error = error\n",
    "        \n",
    "    return PE, s, prev_error\n",
    "\n",
    "def fit_datapoints(datapoints, precision=0.001, precision_newton=0.05):\n",
    "    res = fit_curve_newton_step(datapoints, delta=precision_newton)\n",
    "    \n",
    "    if not res:\n",
    "        return None\n",
    "    \n",
    "    PE, s, error = res\n",
    "    stdev = (error/len(datapoints))**(1/2)\n",
    "    curves = []\n",
    "    \n",
    "    curve_diffs = (datapoints[1:, :] - datapoints[:-1, :]) # Smarter way to do this?\n",
    "    distances = (curve_diffs[:, 0]**2 + curve_diffs[:, 1]**2)**(1/2)\n",
    "    \n",
    "    abs_diffs = datapoints[0] - datapoints[-1]\n",
    "    abs_dist = (abs_diffs[0]**2 + abs_diffs[1]**2)**(1/2)\n",
    "    \n",
    "    if stdev > precision or (np.sum(distances) / abs_dist) > 3:\n",
    "        split = split_datapoints(datapoints)\n",
    "        \n",
    "        if not split:\n",
    "            curves.append([PE, s, stdev, datapoints])\n",
    "        else:\n",
    "            first_h, second_h = split\n",
    "            res_f = fit_datapoints(first_h, precision, precision_newton)\n",
    "            res_s = fit_datapoints(second_h, precision, precision_newton)\n",
    "            \n",
    "            if res_f and res_s:\n",
    "                curves.extend(res_f)\n",
    "                curves.extend(res_s)\n",
    "            else:\n",
    "                curves.append([PE, s, stdev, datapoints])\n",
    "    else:\n",
    "        curves.append([PE, s, stdev, datapoints])\n",
    "        \n",
    "    return curves\n",
    "\n",
    "def get_control_points(P):\n",
    "    C = makeSMatrix(np.array([0, 1]), 4)@P\n",
    "    p0 = C[0, :2]\n",
    "    p3 = C[-1, :2]\n",
    "\n",
    "    PE31d = P[1:, :] * [[1], [2], [3]]\n",
    "    C1d = makeSMatrix(np.array([0, 1]), 3)@PE31d # First derivates\n",
    "\n",
    "    p1 = p0 + (1/3) * C1d[0, :2]\n",
    "    p2 = p3 - (1/3) * C1d[-1, :2]\n",
    "\n",
    "    return [p0, p1, p2, p3]\n",
    "    \n",
    "\n",
    "def parameterize_curve(P, p, debug=False):\n",
    "    p0, p1, p2, p3 = get_control_points(P)\n",
    "    \n",
    "    vec_14 = p3 - p0 # Vec from controlpoint 1 to control point 4\n",
    "    vec_41 = p0 - p3 # Vec from controlpoint 4 to control point 1\n",
    "    \n",
    "    distance_endpoints = np.sum((p3 - p0)**2)**(1/2)\n",
    "\n",
    "    control_vec1 = p1 - p0\n",
    "    control_vec2 = p2 - p3\n",
    "\n",
    "    d1 = np.sum(control_vec1**2)**(1/2) / distance_endpoints\n",
    "    d2 = np.sum(control_vec2**2)**(1/2) / distance_endpoints\n",
    "    \n",
    "    a1 = np.arctan2(\n",
    "        control_vec1[0] * vec_14[1] - control_vec1[1] * vec_14[0],\n",
    "        np.dot(vec_14, control_vec1)\n",
    "    )\n",
    "    \n",
    "    a2 = np.arctan2(\n",
    "        control_vec2[0] * vec_41[1] - control_vec2[1] * vec_41[0],\n",
    "        np.dot(vec_41, control_vec2)\n",
    "    )\n",
    "\n",
    "    if debug:\n",
    "        plt.title(\"angle 1\")\n",
    "        plt.plot([0, vec_14[0]], [0, vec_14[1]], color=\"r\")\n",
    "        plt.plot([0, control_vec1[0]], [0, control_vec1[1]], color=\"b\")\n",
    "        plt.show()\n",
    "    \n",
    "        plt.title(\"angle 2\")\n",
    "        plt.plot([0, vec_41[0]], [0, vec_41[1]], color=\"r\")\n",
    "        plt.plot([0, control_vec2[0]], [0, control_vec2[1]], color=\"b\")\n",
    "        plt.show()\n",
    "\n",
    "    return [vec_14[0], vec_14[1], d1, d2, a1, a2, P[1, 2], P[2, 2], P[3, 2], p]\n",
    "\n",
    "def length_vecs(vec):\n",
    "    return (vec[:, 0]**2 + vec[:, 1]**2)**(1/2)\n",
    "\n",
    "def dot_vecs(vec1, vec2):\n",
    "    return vec1[:, 0]*vec2[:, 0] + vec1[:, 1]*vec2[:, 1]\n",
    "\n",
    "def calc_angles(stroke):\n",
    "    D = stroke[stroke[:, 2] >= 0]\n",
    "    vecs_back = D[1:, :] - D[:-1, :]\n",
    "    vecs_forward = D[:-1, :] - D[1:, :]\n",
    "\n",
    "    frac = dot_vecs(vecs_forward[1:], vecs_back[:-1]) / (length_vecs(vecs_forward[1:]) * length_vecs(vecs_back[:-1]))\n",
    "    \n",
    "    frac[frac < -1] = -1 # TODO: why is this needed?\n",
    "    \n",
    "    return np.arccos(\n",
    "                frac\n",
    "            )\n",
    "\n",
    "def split_datapoints(stroke):\n",
    "#     print(\"split\")\n",
    "    angles = calc_angles(stroke)\n",
    "    indices = np.argsort(angles) + 1\n",
    "    \n",
    "    for index in indices:\n",
    "        if 3 <= index or index <= len(stroke) - 3: # Make sure there are enough datapoints to make the fit.\n",
    "            return stroke[:index+1], stroke[index:]\n",
    "    \n",
    "    return None\n",
    "\n",
    "def stitch_curves(fitted_curves, precision=0.001):\n",
    "    curves = []\n",
    "    \n",
    "    for i, _ in enumerate(fitted_curves[:-1]):\n",
    "        d = np.vstack((fitted_curves[i][3][:-1], fitted_curves[i+1][3]))\n",
    "        PE, s, error = fit_curve_newton_step(d)\n",
    "        stdev = (error/len(d))**(1/2)\n",
    "        \n",
    "        curve_diffs = (d[1:, :] - d[:-1, :]) # Smarter way to do this?\n",
    "        distances = (curve_diffs[:, 0]**2 + curve_diffs[:, 1]**2)**(1/2)\n",
    "    \n",
    "        abs_diffs = d[0] - d[-1]\n",
    "        abs_dist = (abs_diffs[0]**2 + abs_diffs[1]**2)**(1/2)\n",
    "    \n",
    "        if stdev > precision or (np.sum(distances) / abs_dist) > 3:\n",
    "            curves.append(fitted_curves[i])\n",
    "            if i == len(fitted_curves) - 2:\n",
    "                curves.append(fitted_curves[i+1])\n",
    "        else:\n",
    "            if i == len(fitted_curves) - 2:\n",
    "                curves.append([PE, s, stdev, d])\n",
    "            else:\n",
    "                fitted_curves[i+1] = [PE, s, stdev, d]\n",
    "                \n",
    "    return curves\n",
    "\n",
    "def convert_stroke_to_bezier_curves(datapoints):\n",
    "    fitted_curves = fit_datapoints(datapoints)\n",
    "    \n",
    "    if not fitted_curves:\n",
    "        return None\n",
    "\n",
    "    stiched_curves = stitch_curves(fitted_curves)    \n",
    "    parameters = [parameterize_curve(PE, 1) for (PE, s, stdev, d) in stiched_curves]\n",
    "    return parameters\n",
    "\n",
    "def strokes_to_bezier(strokes):\n",
    "    points = []\n",
    "\n",
    "    for stroke in strokes:\n",
    "        res = convert_stroke_to_bezier_curves(stroke)\n",
    "        \n",
    "        if res:\n",
    "            points.extend(res)\n",
    "        \n",
    "    return np.array(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a512db",
   "metadata": {},
   "outputs": [],
   "source": [
    "rtp_features = []\n",
    "bezier_features = []\n",
    "target = []\n",
    "\n",
    "for i, textfile_path in enumerate(textfile_paths):\n",
    "    if i % 100 == 0:\n",
    "        print(i/len(textfile_paths))\n",
    "\n",
    "    textline_strokefile_paths = get_stroke_paths(textfile_path)\n",
    "    \n",
    "    if not textline_strokefile_paths:\n",
    "        continue\n",
    "\n",
    "    lines = [encode_textline(line, alphabet) for line in get_file_lines(textfile_path)]\n",
    "    rtp_strokes = []\n",
    "    bezier_strokes = []\n",
    "    \n",
    "    for textfile_stroke_path in textline_strokefile_paths:\n",
    "        n_strokes = normalize_strokes(read_file(textfile_stroke_path))\n",
    "        \n",
    "        rtp_strokes.append(add_extra_params(resample_strokes(n_strokes)))\n",
    "#         bezier_strokes.append(strokes_to_bezier(n_strokes))\n",
    "\n",
    "    rtp_features.extend(rtp_strokes)\n",
    "#     bezier_features.extend(bezier_strokes)\n",
    "    target.extend(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa86d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(rtp_features), len(target))\n",
    "\n",
    "s = 0\n",
    "\n",
    "for elem in rtp_features:\n",
    "    s+= len(elem)\n",
    "    \n",
    "print(s/len(rtp_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5539349",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = 0\n",
    "\n",
    "def plot_stroke(stroke):\n",
    "    plt.scatter(stroke[:, 0][stroke[:, 4] == 1], stroke[:, 1][stroke[:, 4] == 1])\n",
    "    plt.plot(stroke[:, 0][stroke[:, 2] >= 0], stroke[:, 1][stroke[:, 2] >= 0])\n",
    "    plt.show()\n",
    "    \n",
    "plot_stroke(rtp_features[sample])\n",
    "\n",
    "print(decode_textline(target[sample], alphabet))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b90395",
   "metadata": {},
   "source": [
    "## Storing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776b424a",
   "metadata": {},
   "source": [
    "The data is then padded and stored as compressed numpy files using np.save(). The alphabet is also stored for later use when training the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d74bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_data(l, value=0, width=None):\n",
    "    max_len = max(len(item) for item in l)\n",
    "    \n",
    "    if width:\n",
    "        padded_numpy_array = np.full((len(l), max_len, width), value)\n",
    "    else:\n",
    "        padded_numpy_array = np.full((len(l), max_len), value)\n",
    "    \n",
    "    for i, row in enumerate(l):\n",
    "        padded_numpy_array[i, :len(row)] = row\n",
    "        \n",
    "    return padded_numpy_array\n",
    "        \n",
    "padded_rtp_features = pad_data(rtp_features, width=5)\n",
    "padded_target = pad_data(target, value=len(alphabet))\n",
    "\n",
    "print(padded_rtp_features.shape, padded_target.shape)\n",
    "\n",
    "np.save(\"../../data/processed_data/rtp_features_padded\", padded_rpt_features)\n",
    "np.save(\"../../data/processed_data/target_padded\", padded_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afeac04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"../../data/processed_data/alphabet\", \"wb\") as f:\n",
    "    pickle.dump(alphabet, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a322e301",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806d82ee",
   "metadata": {},
   "source": [
    "For fitting the Bezier curves there are two parameters that can be tuned. Firstly, the necessary precision of a fit that is needed, such that the data points for a certain fit are not split up, we will refer to this as parameter as $\\alpha_1$. Secondly, the precision of the fit that is accurate enough that no more Newton steps need to be taken. Some experiments have been setup to see how these parameters influence the accuracy of the overall fits and what the trade off is between precision of the fits and the number of curves that is needed to achieve that precision, we will refer to this parameter as $\\alpha_2$. In these experiments different values for each parameter have been tested on a sample of the IAM-OnDB data set, we look at the total number of data points needed to represent the input and the precision of the overall fits of all the curves for a stroke. To measure accuracy, we do not parameterize the fitted curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34384083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "testing_set = random.sample(textfile_paths, 5)\n",
    "\n",
    "def convert_stroke_to_bezier_curves_non_parameterized(datapoints, a1, a2):\n",
    "    fitted_curves = fit_datapoints(datapoints, precision=a1, precision_newton=a2)\n",
    "    \n",
    "    if not fitted_curves:\n",
    "        return None\n",
    "\n",
    "    stiched_curves = stitch_curves(fitted_curves, precision=a1)\n",
    "    return [stdev for (PE, s, stdev, d) in stiched_curves]\n",
    "\n",
    "def strokes_to_bezier_non_parameterized(strokes, a1, a2):\n",
    "    points = []\n",
    "\n",
    "    for stroke in strokes:\n",
    "        res = convert_stroke_to_bezier_curves_non_parameterized(stroke, a1, a2)\n",
    "        \n",
    "        if res:\n",
    "            points.extend(res)\n",
    "        \n",
    "    return np.array(points)\n",
    "\n",
    "def fit_testing_set(testing_set, a1, a2):\n",
    "    bezier_features = []\n",
    "\n",
    "    for i, textfile_path in enumerate(testing_set):\n",
    "        if i % 100 == 0:\n",
    "            print(i/len(textfile_paths))\n",
    "\n",
    "        textline_strokefile_paths = get_stroke_paths(textfile_path)\n",
    "    \n",
    "        if not textline_strokefile_paths:\n",
    "            continue\n",
    "\n",
    "        lines = [encode_textline(line, alphabet) for line in get_file_lines(textfile_path)]\n",
    "        rtp_strokes = []\n",
    "        bezier_strokes = []\n",
    "    \n",
    "        for textfile_stroke_path in textline_strokefile_paths:\n",
    "            n_strokes = normalize_strokes(read_file(textfile_stroke_path))\n",
    "            bezier_strokes.append(strokes_to_bezier_non_parameterized(n_strokes, a1, a2))\n",
    "\n",
    "        bezier_features.extend(bezier_strokes)\n",
    "        \n",
    "    return bezier_features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
