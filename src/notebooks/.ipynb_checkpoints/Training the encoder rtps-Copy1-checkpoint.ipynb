{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e702dcc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11983, 1713, 5) (11983, 64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "padded_features = np.load(\"../../data/processed_data/rtp_features.npy\")\n",
    "padded_target = np.load(\"../../data/processed_data/rtp_target.npy\")\n",
    "\n",
    "with open(\"../../data/processed_data/alphabet\", \"rb\") as f:\n",
    "    alphabet = pickle.load(f)\n",
    "\n",
    "print(padded_features.shape, padded_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12630381",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b619fe7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.9.0\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "print(tf.test.is_built_with_cuda())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c52bc5",
   "metadata": {},
   "source": [
    "link 6 from google paper\n",
    "\n",
    "https://link.springer.com/content/pdf/10.1007/978-3-662-44415-3.pdf\n",
    "\n",
    "page 404 uses merge mode sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "523333b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpad_data(data, comp_val):\n",
    "    unpadded_data = []\n",
    "    \n",
    "    for elem in data:\n",
    "        t = len(elem) - np.sum(np.all(elem == comp_val, axis=-1))\n",
    "        unpadded_data.append(elem[:t].tolist())\n",
    "    \n",
    "    return unpadded_data\n",
    "\n",
    "def split_data(features, target, train_size=0.9):\n",
    "    size = len(features)\n",
    "    indices = np.arange(size)\n",
    "    np.random.shuffle(indices)\n",
    "    train_samples = int(size * train_size)\n",
    "    \n",
    "    x_train, y_train = features[indices[:train_samples]], target[indices[:train_samples]]\n",
    "    x_valid, y_valid = features[indices[train_samples:]], target[indices[train_samples:]]\n",
    "    \n",
    "    return (\n",
    "        x_train,\n",
    "        x_valid,\n",
    "        y_train,\n",
    "        y_valid,\n",
    "    )\n",
    "    \n",
    "#     return (\n",
    "#         unpad_data(x_train, np.repeat(0, 5)),\n",
    "#         unpad_data(x_valid, np.repeat(0, 5)),\n",
    "#         unpad_data(y_train, len(alphabet)),\n",
    "#         unpad_data(y_valid, len(alphabet))\n",
    "#     )\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = split_data(padded_features, padded_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc93ff2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10784 1713 5\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train), len(x_train[0]), len(x_train[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf601632",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_SIZE = len(alphabet) + 1\n",
    "\n",
    "N_BLSTM_LAYERS = 5\n",
    "N_CELLS = 64\n",
    "\n",
    "LEARNING_RATE = 10**-4\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "N_FEATURES = 5\n",
    "N_TIMESTEPS = 1713\n",
    "N_EPOCHS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "34336f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Bidirectional, LSTM, Dense, Input\n",
    "\n",
    "def build_model(n_blstm_layers, n_cells, n_features, output_size):\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    for i in range(n_blstm_layers-1):\n",
    "        model.add(Bidirectional(LSTM(n_cells,\n",
    "                                     input_shape=(None, n_features),\n",
    "                                     return_sequences = True,\n",
    "                                     dropout = 0.5),\n",
    "                                merge_mode = 'sum'))\n",
    "    \n",
    "    model.add(Bidirectional(LSTM(N_CELLS,\n",
    "                                 input_shape=(None, n_features),\n",
    "                                 return_sequences = True,\n",
    "                                 dropout = 0.5),\n",
    "                            merge_mode = 'sum'))\n",
    "\n",
    "    model.add(Dense(output_size, activation = 'softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d14c905",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_loss(y_true, y_pred):\n",
    "    batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n",
    "    input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n",
    "    label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n",
    "\n",
    "    input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "    label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "\n",
    "    loss = keras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4ac82452",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(N_BLSTM_LAYERS, N_CELLS, N_FEATURES, OUTPUT_SIZE)\n",
    "\n",
    "model.compile(\n",
    "    loss=ctc_loss,\n",
    "    optimizer= tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "54760b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(train_data, valid_data, n_epochs, batch_size):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((train_data, valid_data))\n",
    "    dataset = dataset.shuffle(1000)\n",
    "    dataset = dataset.repeat(n_epochs)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "#     dataset = dataset.bucket_by_sequence_length(\n",
    "#         element_length_func=lambda t, v: tf.shape(t)[0],\n",
    "#         bucket_boundaries=np.repeat(len(train_data[0][0]), (len(train_data[0])/batch_size)-1),\n",
    "#         bucket_batch_sizes=np.repeat(batch_size, len(train_data[0])/batch_size),\n",
    "#         pad_to_bucket_boundary=False\n",
    "#     )\n",
    "    dataset = dataset.prefetch(1)\n",
    "    return dataset\n",
    "\n",
    "train_dataset = create_dataset(x_train, y_train, N_EPOCHS, BATCH_SIZE)\n",
    "validation_dataset = create_dataset(x_valid, y_valid, N_EPOCHS, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d7dde094",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=validation_dataset,\n",
    "    epochs = N_EPOCHS,\n",
    "    batch_size = BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe22bb7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
